{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trajectory Recommendation with Active Learning\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experimental Protocol for Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Traj$: a trajectory\n",
    "- $|Traj|$: the number of POIs in trajectory $Traj$\n",
    "- $P_s$: the `start` (first) POI of a trajectory\n",
    "- $P_e$: the `end` (last) POI of a trajectory\n",
    "- $u$: a specific user\n",
    "- $\\textbf{x}$: an example\n",
    "- $\\textbf{y}$: label of an example $\\textbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Learning without personalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example: $\\textbf{x} = (P_s, P_e, |Traj|)$\n",
    "- Label of Example: $\\textbf{y} = Traj$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split dataset into training set(~10%), annotation set (i.e. simulate user annotation)(~50%), evaluation set (a.k.a. test set) (~40%).\n",
    "\n",
    "1. Learning parameters (of a probabilistic model) using the above training set, evaluate its performance (e.g. calculate F1 measure) on evaluation set.\n",
    "\n",
    "1. Query unlabelled example (in annotation set) using a query strategy (e.g. Information Density, Least Confident as recommended in [Settles08](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.7401&rep=rep1&type=pdf)), add the label of queried example to training set, reestimate parameters and evaluate its performance on evaluation set.\n",
    "\n",
    "1. Possible k-fold cross validation using all data in training set + evaluation set (~50%).\n",
    "\n",
    "1. Draw a learning curve: F1 vs. #query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Markov Chain combining POI category transition probabilities, POI popularity transition probabilities and POI pair distance transition probabilities, all estimated using Maximumu Likelihood (MLE).\n",
    "- A Markov Chain only with POI-POI transition matrix learned using MLE (hard to deal with sparsity) or factorization technique (capable of dealing with sparsity) described in [FPMC paper](http://ramb.ethz.ch/CDstore/www2010/www/p811.pdf).   \n",
    "  How to incorporate POI-pair distance transitions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The same probabilistic model with a random example choosen (of a random user) in the query step (passive learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Learning personalized recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example: $\\textbf{x} = (P_s, P_e, |Traj|, u)$\n",
    "- Label of Example: $\\textbf{y} = Traj$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For all data (travelling sequences) of each user, split it into training set(~10%), annotation set (i.e. simulate user annotation)(~50%), evaluation set (~40%).\n",
    "\n",
    "1. Learning a transition tensor using factorization technique described in [FPMC paper](http://ramb.ethz.ch/CDstore/www2010/www/p811.pdf) with all data in training set, evaluate its performance (e.g. calculate F1 measure) on evaluation set.\n",
    "\n",
    "1. Query unlabelled example (in annotation set) using a query strategy (e.g. Information Density, Least Confident as recommended in [Settles08](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.7401&rep=rep1&type=pdf)), add the label of queried example to training set, reestimate parameters and evaluate its performance on evaluation set.\n",
    "\n",
    "1. Possible k-fold cross validation (probably have to due to high sparsity data for each user) using all data in training set + evaluation set (~50%).\n",
    "\n",
    "1. Draw a learning curve: F1 vs. #query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A transition tensor estimated using factorisation techniques described in [FPMC paper](http://ramb.ethz.ch/CDstore/www2010/www/p811.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The same probabilistic model with a random example choosen (of a specific user) in the query step (passive learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible Drawbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cost of reestimating parameters after each query using techniques described in [FPMC paper](http://ramb.ethz.ch/CDstore/www2010/www/p811.pdf) is very expensive.   \n",
    "  Investigate if incremental paramter update is possible.\n",
    "- How to incorporate POI-pair distance transitions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1.4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Query Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following query strategies are adapted from sequence labeling tasks describe in [Settles08](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.7401&rep=rep1&type=pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sequence Entropy**\n",
    "\\begin{equation}\n",
    "\\phi^{SE}(\\textbf{x}) = - \\sum_{\\hat{\\textbf{y}}} P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta) \n",
    "\\log(P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta))\n",
    "\\end{equation}\n",
    "where $\\hat{\\textbf{y}}$ ranges over all possible labels for example $\\textbf{x}$.  \n",
    "Note that the number of possible labels grows exponentially with $|Traj|$ in $\\textbf{x}$, to make computation feasible,\n",
    "[Kim06](http://www.aclweb.org/anthology/N06-2018) used the $N$-best possible labels to approximate, concretely, \n",
    "define **N-best Sequence Entropy** as\n",
    "\\begin{equation}\n",
    "\\phi^{NSE}(\\textbf{x}) = - \\sum_{\\hat{\\textbf{y}} \\in \\mathcal{N}} P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta) \n",
    "\\log(P(\\hat{\\textbf{y}} | \\textbf{x}; \\Theta))\n",
    "\\end{equation}\n",
    "where $\\mathcal{N} = \\{\\textbf{y}_1^*, \\dots, \\textbf{y}_N^*\\}$ is the set of the $N$ most likely labels of example $\\textbf{x}$.  \n",
    "This query strategy select the example $\\textbf{x}$ of maximum $\\phi^{SE}$ or $\\phi^{NSE}$ from all unlabelled examples in a pool to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Information Density**\n",
    "\\begin{equation}\n",
    "\\phi^{ID}(\\textbf{x}) = \\phi^{SE}(\\textbf{x}) \\times \n",
    "\\left(\n",
    "\\frac{1}{U} \\sum_{u=1}^U \\text{sim}(\\textbf{x}, \\textbf{x}^u)\n",
    "\\right)^\\beta\n",
    "\\end{equation}\n",
    "That is, the informativeness of example $\\textbf{x}$ is weighted by its average similarity to all other unlabelled examples (denoted by $\\mathcal{U})$ in the pool, subject to parameter $\\beta$ which was set to $1$ in [Settles08](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.7401&rep=rep1&type=pdf) giving no reason, \n",
    "sequence entropy $\\phi^{SE}$ measures the \"base\" informativeness and could be replaced by $\\phi^{NSE}$ defined above.  \n",
    "This query strategy select the example $\\textbf{x}$ of maximum $\\phi^{ID}$ from all unlabelled examples in a pool to query.  \n",
    "**TODO**: define a function to compute the simularity between two trajectories, [Settles08](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.7401&rep=rep1&type=pdf) uses cosine simularity after transforming a sequence into a fixed length feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Least Confident**\n",
    "\\begin{equation}\n",
    "\\phi^{LC}(\\textbf{x}) = 1 - P(\\textbf{y}^* | \\textbf{x}; \\Theta)\n",
    "\\end{equation}\n",
    "where $\\textbf{y}^*$ is the most likely label of example $\\textbf{x}$ with respect to a probabilistic model of which the parameters are denoted by $\\Theta$.  \n",
    "This query strategy select the example $\\textbf{x}$ of maximum $\\phi^{LC}$ from all unlabelled examples in a pool to query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1.5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating the performance of the recommender, given \n",
    "- a probabilistic model\n",
    "- an example $\\textbf{x}$: $(P_s, P_e, |Traj|)$ (with user $u$ in the personalized setting)\n",
    "\n",
    "Predict the label of $\\textbf{x}$, i.e. recommend a trajectory \n",
    "- satisfy constraints specified in $\\textbf{x}$, i.e. starts at $P_s$, ends at $P_e$, with $|Traj|$ POIs, for user $u$ in the personalized setting\n",
    "- with highest posterior probability with respect to the probabilistic model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision, Recall, F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute `Precision`, `Recall` and `F1-score` defined [here](./ijcai15.ipynb#sec2.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate POI visit duration in recommended trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Estimate POI visit duration as the average visit duration of the same POI category\n",
    "  - (average over) all users (possible inaccuracy) \n",
    "  - (average over) the specific user (possible sparsity)\n",
    "- Compute the mean square error of POI visit durations in recommended trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulate trajectory recommendation as an active learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example: a tuple (user, trajectory)\n",
    "- Label of example: binary (i.e. positive/negative)\n",
    "   - Observed tuples (user, trajectory) (i.e. those we extracted from data) are labelled as positive\n",
    "   - Unobserved tuples (e.g. trajectories generated on the fly such as synthesize trajectories by enumeration) are unlabelled\n",
    "   - Tuples chosen to query a user will be labelled as positive/negative if the feedback from the user is positive/negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query strategies: \n",
    "given (start, end) and trajectory length $l$ for a specific user $u$, we'll select an example for user $u$ to label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute/enumerate all trajectories of length $l$ with (start, end) as candidates\n",
    "- Use a uniform prior for all candidates\n",
    "- Compute the likelihood of candidates as described [here](./traj_MC.ipynb#sec4)\n",
    "- Sort candidates by their posterior probabilities (i.e. $\\text{prior} \\times \\text{likelihood}$) in descending order\n",
    "- Select a trajectory from the top $K$ (e.g. 5) candidates with probability proportional to its posterior for user $u$ to label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: utilise user-specific features when selecting trajectory for user $u$ to label if there are trajectories of user $u$ in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandit Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulate trajectory recommendation as a contextual multi-armed bandit problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Arm: transition from current POI$_i$ to another POI$_j$, (allow sub-tours?) (encode the target user into each arm?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Context: transition features including POI category transition probabilities, POI popularity transition probabilities, distance of POI pair transition probabilities and user specific features (visit duration/frequency), available in [this notebook ](./traj_feature.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Reward: NOT clear yet, but could be one of the followings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [F1-scored](./ijcai15.ipynb#sec2.1) based metric, e.g. let the trajectory up to time $t$ (after choosing a certain arm) is `[p1, p2, p3]`, ground truth trajectories `[p1, p2, p3]` and `[p1, p2, p4]`, the reward is computed as \n",
    "\\begin{equation}\n",
    "max\\{F1([p1, p2, p3], [p1, p2, p3]), F1([p1, p2, p3], [p1, p2, p4])\\} = max\\{1.0, 0.667\\} = 1.0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visiting order based metric, e.g. let the trajectory up to time $t$ (after choosing a certain arm) is `[p1, p3, p2]`, ground truth trajectories `[p1, p2, p3]` and `[p1, p2, p4]`, \n",
    "with respect to trajectory `[p1, p2, p3]`, pair `(p1, p3)` in `[p1, p3, p2]` count because `p3` is visited after `p1`,\n",
    "similarly, pair `(p1, p2)` count but pair `(p3, p2)` does not count because `p3` is not visited before `p2` in trajectory `[p1, p2, p3]`, so the total number of pairs that counts is `2` and there are `3` pairs in total, the metric here is `2/3`. Similarly, the metric with respect to trajectory `[p1, p2, p4]` is `1/3` because there is only one pair, i.e. `(p1, p2)` among the `3` pairs count. The reward is computed as \n",
    "\\begin{equation}\n",
    "max\\{\\frac{2}{3}, \\frac{1}{3}\\} = \\frac{2}{3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metrics incorporating POI category, POI popularity, and the travel distance from the current POI, e.g. let the trajectory up to time $t$ (after choosing a certain arm) is `[p1, p3]`, and ground truth trajectory `[p1, p4]`, \n",
    "if `p3` and `p4` are of the same category as well as similar popularity (i.e. belong to the same popularity class after discretizing the POI popularity) and similar distance (i.e. same distance class after discretization) from the current POI, then the reward of arm `(p1, p3)` would be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Regret: regret after $T$ trials is defined as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "R(T) = \\textbf{E}\\left[\\sum_t^T(\\text{optimal reward})_t\\right] - \\textbf{E}\\left[\\sum_t^T(\\text{actual reward})_t\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Sampling strategies: NOT clear yet, but could be one of the followings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\epsilon$-greedy: choose arm (i.e. transition) with the highest posterior with probability 1-$\\epsilon$, choose a random arm with probability $\\epsilon$. (using a uniform prior and compute likelihood as describe [here](./traj_MC.ipynb#sec4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thompson sampling: design a prior of arm's reward (uniform? gaussian? etc.), compute likelihood as described [here](./traj_MC.ipynb#sec4). an interesting [paper on Thompson sampling](http://www.research.rutgers.edu/~lihon/pub/Chapelle12Empirical.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UCB-type strategies such as LinUCB introduced in [this paper](http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior knowledge only\n",
    "\n",
    "Given (start, end) and trajectory length $l$ for a specific user $u$, we'll recommend a trajectory to user $u$ as follows:\n",
    "- Estimate transition probabilities using all data\n",
    "- Compute/enumerate all trajectories of length $l$ with (start, end) as candidates\n",
    "- Use a uniform prior for all candidates\n",
    "- Compute the likelihood of candidates as described [here](./traj_feature.ipynb#sec4)\n",
    "- Sort candidates by their posterior probabilities (the same as the likelihood since the prior is uniform) in descending order\n",
    "- Recommend a trajectory with probability proportional to its posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  User specific knowledge\n",
    "\n",
    "Given (start, end) and trajectory length $l$ for a specific user $u$, we'll recommend a trajectory to user $u$ as follows:\n",
    "- Estimate transition probabilities using per user data\n",
    "- Compute/enumerate all trajectories of length $l$ with (start, end) as candidates\n",
    "- Use a uniform prior for all candidates\n",
    "- Compute the likelihood of candidates as described [here](./traj_feature.ipynb#sec4)\n",
    "- Sort candidates by their posterior probabilities (the same as the likelihood since the prior is uniform) in descending order\n",
    "- Recommend a trajectory with probability proportional to its posterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment for choosing formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same leave-one-out cross validation approach described in the [ijcai15 paper](#https://sites.google.com/site/limkwanhui/publications/2015-IJCAI-personalTour.pdf?attredirects=0) as the basis to measure the performance of different formulations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, for each user $u$ in the dataset, choose one trajectory (length >= 3) from all trajectories of $u$ uniformly at random, this trajectory is used as the ground truth to measure the performance of the recommendation (i.e. compute the precision, recall or F1-score defined [here](./ijcai15.ipynb#sec2.1)), all other trajectories are used to train/estimate parameters. The experiment will iterate through all users and compute a list of [precision, recall and F1-score](./ijcai15.ipynb#sec2.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation the performance of Active Learning formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the active learning setting, we'll get a metric (i.e. mean of precision, recall or F1-score as we do cross validation for each user) for each number of examples queried, thus, the result of experiment is a learning curve: `(mean) precision/recall/F1-score` vs. `#example queried`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation the performance of Bandit formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bandit setting, we'll generally get a `regret` vs. `#trial` curve, where the `regret` is the cumulative regret defined [above](#sec1.2), and `#trail` is the number of trials performed (i.e. choose an arm and get corresponding reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation the performance of the Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the baseline setting, we'll get a metric (i.e. precision, recall or F1-score) for each validation trajectory, thus, the result is a list of numeric (i.e. a list of precision, recall or F1-score) which can be seen as a number of samples drawn from an unknown distribution that characterises the performance of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is straightforward to compare the performance between the Active Learning and the baseline formulation, for example,\n",
    "- Simply use the mean of results in the baseline setting, we get a single number (e.g. mean precision/recall/F1-score) to measure the performance of the baseline formulation\n",
    "- Simply use the mean of results for each number of examples queried in the active learning setting, we get a list of points (#example queried, metric) to measure the performance of the active learning formulation\n",
    "- Or we can use all numbers instead of just their mean and utilise KL divergence to measure the performance difference.\n",
    "\n",
    "However, in the bandit setting, we need to convert `regret` to precision/recall/F1-score to ease the comparison between the bandit setting and other forumations. (how to convert?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
